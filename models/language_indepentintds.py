# -*- coding: utf-8 -*-
"""language indepent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c1pz6ML0EldXOh5lsfz7iuTVPIw5o8U-
"""

#!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7

#pip install transformers torch datasets scikit-learn

import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import AdamW, get_scheduler
import numpy as np
from tqdm import tqdm

path = 'lear_eng&mar.xlsx'
dataset = pd.read_excel(path)



#dataset.head(1)

# Load the XLM-RoBERTa tokenizer
tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')

from sklearn.model_selection import train_test_split

# Step 1: First split - Train (80%) and Temp (20%) [Temp will be split into validation and test]
train_df, temp_df = train_test_split(dataset, test_size=0.3, random_state=42, stratify=dataset['language'])

# Step 2: Second split - Validation (10%) and Test (10%) from Temp
val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['language'])

class EssayDataset(Dataset):
    def __init__(self, texts, scores, languages, tokenizer, max_length=512):
        self.encodings = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')
        self.scores = torch.tensor(scores, dtype=torch.float)
        self.languages = languages  # List of language tags (e.g., ['english', 'hindi', 'marathi'])

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item['labels'] = self.scores[idx]
        item['language'] = self.languages[idx]  # Include language tag in each batch item
        return item

    def __len__(self):
        return len(self.scores)

# Assuming your dataframe has 'text', 'score', and 'language' columns
train_texts = train_df['text'].tolist()
train_scores = train_df['score'].tolist()
train_languages = train_df['language'].tolist()

val_texts = val_df['text'].tolist()
val_scores = val_df['score'].tolist()
val_languages = val_df['language'].tolist()

test_texts = test_df['text'].tolist()
test_scores = test_df['score'].tolist()
test_languages = test_df['language'].tolist()

# Create datasets
train_dataset = EssayDataset(train_texts, train_scores, train_languages, tokenizer)
val_dataset = EssayDataset(val_texts, val_scores, val_languages, tokenizer)
test_dataset = EssayDataset(test_texts, test_scores, test_languages, tokenizer)

from torch.utils.data import DataLoader

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)

# Check the first batch
for batch in train_loader:
    print(batch.keys())  # Should include 'input_ids', 'attention_mask', 'labels', and 'language'
    print(batch['language'])  # Verify that this prints a list of language tags (e.g., ['english', 'hindi'])
    break

# Load XLM-RoBERTa model for sequence classification (regression task)
model = AutoModelForSequenceClassification.from_pretrained('ai4bharat/indic-bert',num_labels=1)

optimizer = AdamW(model.parameters(),lr = 2e-5)

num_epochs = 8
num_training_steps = len(train_loader) * num_epochs
lr_scheduler = get_scheduler(
    "linear",
    optimizer = optimizer,
    num_warmup_steps = 0,
    num_training_steps = num_training_steps
)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1}/{num_epochs}"):
        # Move only tensor items to the device
        inputs = {k: v.to(device) for k, v in batch.items() if k != 'language'}
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(**inputs)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        lr_scheduler.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_loss:.4f}")

val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)
from sklearn.metrics import cohen_kappa_score

# Validation Loop
model.eval()
val_preds, val_labels, val_languages = [], [], []

with torch.no_grad():
    for batch in tqdm(val_loader, desc="Validation"):
        inputs = {k: v.to(device) for k, v in batch.items() if k != 'language'}
        labels = batch['labels'].to(device)

        outputs = model(**inputs)
        preds = outputs.logits.squeeze().cpu().numpy()
        labels = labels.cpu().numpy()
        languages = batch['language']  # List of language tags

        val_preds.extend(preds)
        val_labels.extend(labels)
        val_languages.extend(languages)

# Round predictions to the nearest integer for QWK calculation
val_preds_rounded = np.round(val_preds)
val_labels_rounded = np.round(val_labels)

# Calculate Overall QWK Score
overall_qwk = cohen_kappa_score(val_labels_rounded, val_preds_rounded, weights='quadratic')
print(f"\nValidation Overall QWK Score: {overall_qwk:.4f}")

# Calculate QWK per Language
unique_languages = set(val_languages)
for lang in unique_languages:
    lang_indices = [i for i, l in enumerate(val_languages) if l == lang]
    lang_preds = [val_preds_rounded[i] for i in lang_indices]
    lang_labels = [val_labels_rounded[i] for i in lang_indices]

    lang_qwk = cohen_kappa_score(lang_labels, lang_preds, weights='quadratic')
    print(f"{lang.capitalize()} Validation QWK Score: {lang_qwk:.4f}")

print(f"Preds Type: {type(preds)}, Preds: {preds}")
print(f"Labels Type: {type(labels)}, Labels: {labels}")

test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)
from sklearn.metrics import cohen_kappa_score, mean_squared_error, r2_score
model.eval()
test_preds, test_labels, test_languages = [], [], []

with torch.no_grad():
    for batch in tqdm(test_loader, desc="Testing"):
        inputs = {k: v.to(device) for k, v in batch.items() if k != 'language'}
        labels = batch['labels'].to(device)

        outputs = model(**inputs)

        # Ensure preds is always a list
        preds = outputs.logits.cpu().numpy()
        preds = np.atleast_1d(preds).tolist()  # Convert to a list even if it's a single float

        labels = labels.cpu().numpy()
        labels = np.atleast_1d(labels).tolist()  # Convert to a list even if it's a single float

        languages = batch['language']

        test_preds.extend(preds)
        test_labels.extend(labels)
        test_languages.extend(languages)

# Round predictions and labels to the nearest integers
test_preds_rounded = np.round(test_preds)
test_labels_rounded = np.round(test_labels)

# Calculate Overall QWK Score
overall_qwk = cohen_kappa_score(test_labels_rounded, test_preds_rounded, weights='quadratic')

# Calculate QWK per Language
unique_languages = set(test_languages)
language_qwk_scores = {}

for lang in unique_languages:
    lang_indices = [i for i, l in enumerate(test_languages) if l == lang]
    lang_preds = [test_preds_rounded[i] for i in lang_indices]
    lang_labels = [test_labels_rounded[i] for i in lang_indices]

    lang_qwk = cohen_kappa_score(lang_labels, lang_preds, weights='quadratic')
    language_qwk_scores[lang.capitalize()] = lang_qwk

# Include overall QWK in the scores
language_qwk_scores['Overall'] = overall_qwk

# Print QWK scores for verification
for lang, score in language_qwk_scores.items():
    print(f"{lang} QWK Score: {score:.4f}")

import matplotlib.pyplot as plt

# Plot QWK scores as a bar graph
plt.figure(figsize=(10, 6))
plt.bar(language_qwk_scores.keys(), language_qwk_scores.values(), color='skyblue')

# Add labels and title
plt.xlabel('Languages')
plt.ylabel('QWK Score')
plt.title('QWK Score Comparison Across Languages')
plt.ylim(0, 1)  # QWK ranges between -1 and 1
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Add QWK score values on top of each bar
for i, (lang, score) in enumerate(language_qwk_scores.items()):
    plt.text(i, score + 0.02, f"{score:.2f}", ha='center', va='bottom', fontsize=12)

# Save the plot as a PNG file
plt.savefig('qwk_score_comparison.png', dpi=300)
plt.show()

# Save the trained model
model.save_pretrained('./indic')
tokenizer.save_pretrained('./indic')

print("Model saved successfully!")
